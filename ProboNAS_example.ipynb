{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **ProboNAS example**"
      ],
      "metadata": {
        "id": "HmrtxdeuMQmS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3tARQvUGkbb"
      },
      "source": [
        "##### Drive and libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_j02IcSRpH4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert inside ***your_path*** the path to the code folder in your drive:"
      ],
      "metadata": {
        "id": "8FMpe5QQNPJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "your_path = \"\""
      ],
      "metadata": {
        "id": "1mqvqyc5NXUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nJ57w1306pB8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "for f in os.listdir('/content/drive/MyDrive/'+your_path+'Notebooks/'):\n",
        "  os.system(f\"cp -r /content/drive/MyDrive/\"+your_path+\"Notebooks/{f} .\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uf4fSoxI5zz-"
      },
      "outputs": [],
      "source": [
        "!pip install -U 'git+https://github.com/facebookresearch/fvcore'\n",
        "!pip install pyvww"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNdgqgWm5m-H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torch.nn import Module\n",
        "from torch.nn import Conv2d\n",
        "from torch.nn import Linear\n",
        "from torch.nn import MaxPool2d\n",
        "from torch.nn import ReLU\n",
        "from torch.nn import LogSoftmax\n",
        "from torch import flatten\n",
        "from torchvision import transforms\n",
        "from torchvision.io import read_image\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import shutil\n",
        "\n",
        "from pyvww.pytorch import VisualWakeWordsClassification\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from invbneckzpadd import InvBNeck\n",
        "\n",
        "from convnextzpadd import ConvNext\n",
        "\n",
        "from Downsampling import Downsampling\n",
        "\n",
        "from createNetStem import Net\n",
        "from measures import *\n",
        "from measures.logsynflow import compute_synflow_per_weight\n",
        "from measures.naswot import compute_naswot_score\n",
        "from fvcore.nn import FlopCountAnalysis\n",
        "from fvcore.nn import parameter_count\n",
        "\n",
        "\n",
        "from randomNetStem import randomGen\n",
        "import random\n",
        "import time\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from genetic_algorithmStem import ProboNAS\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nftAkvIx4Ig"
      },
      "source": [
        "##### Prepare the data folder"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert inside ***train_path*** the path to the tar file that contains the train data in your drive (example: '/content/drive/MyDrive/Project4a/dataset/train2014.tar'):"
      ],
      "metadata": {
        "id": "E_8JuRVWOXVR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJMlNYLUob-C"
      },
      "outputs": [],
      "source": [
        "!mkdir train2014\n",
        "!tar -xf train_path -C train2014"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert inside ***val_path*** the path to the tar file that contains the validation data in your drive (example: '/content/drive/MyDrive/Project4a/dataset/val2014.tar'):"
      ],
      "metadata": {
        "id": "lvu0mZimQLxL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZ5NukuVolqF"
      },
      "outputs": [],
      "source": [
        "!mkdir val2014\n",
        "!tar -xf val_path -C val2014"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vevkmYXtoqJ4"
      },
      "outputs": [],
      "source": [
        "#define a function & pass dst. directory and src. directories\n",
        "def merge_directories(new_directory_name, *directories_to_merge):\n",
        "    if not os.path.exists(new_directory_name):\n",
        "        os.makedirs(new_directory_name) #create a dst. directory if not exist\n",
        "\n",
        "    for directory in directories_to_merge:\n",
        "        for item in os.listdir(directory):  #iterate sub-directory from source folders\n",
        "            #join path of folder and sub-folder\n",
        "            s = os.path.join(directory, item)\n",
        "            d = os.path.join(new_directory_name, item)\n",
        "            if os.path.isdir(s):\n",
        "                if item in os.listdir(new_directory_name):\n",
        "                    files = os.listdir(s)\n",
        "                    for file in files:  #iterate file from sub-folder\n",
        "                        j = os.path.join(s, file)\n",
        "                        k = os.path.join(d, file)\n",
        "                        shutil.copy2(j,k)  #paste file in already existed sub-directory\n",
        "                else:\n",
        "                    shutil.copytree(s, d)  #create a sub-directory in dst directory then paste file\n",
        "            else:\n",
        "                shutil.copy2(s, d)  #paste file in already existed sub-directory\n",
        "\n",
        "!mkdir all2014\n",
        "dst_directory = '/content/all2014'\n",
        "src_directory1 = \"/content/train2014/content/path-to-COCO-dataset/train2014\"\n",
        "src_directory2 = \"/content/val2014/content/path-to-COCO-dataset/val2014\"\n",
        "\n",
        "#call function and pass path of the dst directory & all src directories.\n",
        "merge_directories(dst_directory, src_directory1, src_directory2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PX78bnu2oyWd"
      },
      "outputs": [],
      "source": [
        "folder = '/content/all2014'\n",
        "\n",
        "for file_name in os.listdir(folder):\n",
        "\toldName = os.path.join(folder, file_name)\n",
        "\tn = os.path.splitext(file_name)[0]\n",
        "\tn = n.split('_')[2]\n",
        "\tb = n + '.jpg'\n",
        "\tnewName = os.path.join(folder, b)\n",
        "  # Rename the file\n",
        "\tos.rename(oldName, newName)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SutTmrTvyROQ"
      },
      "source": [
        "##### Create dataset and dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Insert inside ***data_path*** the path to the dataset folder in your drive:"
      ],
      "metadata": {
        "id": "GFfmgadpQzDx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"\""
      ],
      "metadata": {
        "id": "FsYPZLxoRQ1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-d--bFH8us6"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "     transforms.Resize(128)])\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "trainset = VisualWakeWordsClassification(root=\"/content/all2014\",\n",
        "                                         annFile=\"/content/drive/MyDrive/\"+data_path+\"annotations2014/instances_train.json\",\n",
        "                                         transform=transform\n",
        "                                         )\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "testset = VisualWakeWordsClassification(root=\"/content/all2014\",\n",
        "                                        annFile=\"/content/drive/MyDrive/\"+data_path+\"annotations2014/instances_val.json\",\n",
        "                                        transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('person', 'not-person')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yz2J1tQGHnvQ"
      },
      "source": [
        "##### ProboNAS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yFmiPhZ46XC"
      },
      "outputs": [],
      "source": [
        "\"\"\"Singole probabilit√† che geni/blocchi cambino\"\"\"\n",
        "params={\"p_block\":1,\n",
        "        \"p_gene\": 0.3,\n",
        "        \"std_channels\": 5}\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "genetic_code=ProboNAS(N=25, n=5, trainloader=trainloader, max_time=15, max_params=2500000, max_flops=200000000*batch_size, params=params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H45-9U7uLgCX"
      },
      "outputs": [],
      "source": [
        "genetic_code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m9UEsCwrVL-"
      },
      "outputs": [],
      "source": [
        "net=Net(3,2,genetic_code, is_training=True)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = torch.device(\"cpu\")\n",
        "net.to(device)\n",
        "\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.PolynomialLR(optimizer, total_iters = 8, power = 1)\n",
        "\n",
        "losses =[]\n",
        "net.train()\n",
        "for epoch in range(10):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(tqdm(trainloader), 0):\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "        if i %  100 == 99:    # print every 2000 mini-batches\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "dataiter = iter(testloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBPyZT4hr1UX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "y_pred = []\n",
        "y_true = []\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in tqdm(testloader):\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    output = net(inputs) # Feed Network\n",
        "\n",
        "    output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
        "    y_pred.extend(output) # Save Prediction\n",
        "\n",
        "    labels = labels.data.cpu().numpy()\n",
        "    y_true.extend(labels) # Save Truth\n",
        "\n",
        "# constant for classes\n",
        "classes = ('person', 'not-person')\n",
        "\n",
        "# Build confusion matrix\n",
        "cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
        "                     columns = [i for i in classes])\n",
        "plt.figure(figsize = (12,7))\n",
        "sn.heatmap(df_cm, annot=True)\n",
        "plt.savefig('output.png')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "\n",
        "sklearn.metrics.accuracy_score(y_true, y_pred)"
      ],
      "metadata": {
        "id": "rPu7_C_7ye52"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "W3tARQvUGkbb",
        "-nftAkvIx4Ig",
        "SutTmrTvyROQ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}